''' Contains tools to train our model using the mirrored distribution strategy.


Notes
-----
This module contains the actual training function used to train a model
using the built-in Keras model.fit API. It combines the data input pipeline 
defined in projectame_input.py (which uses the .tfrecord files previously
generated by audio_processing.py), the CNN architecture proposed 
by Pon et. al (2018) defined in projectname.py, and a standard training loop 
with mirrored strategy integrated for multiple-GPU training.

The training function tries to optimise BinaryCrossentropy for each tag
prediction (batch loss is summed over the batch size, instead of being averaged), and 
displays the area under the ROC and PR curves as metrics. The optimizer can
be fully specified in the config.json file.

The learning rate is halved whenever the validation AUC PR does not improve
for more than plateau_patience epochs. Moreover, training is automatically
interrupted if the validation AUC PR does not improve for more than early_stop_patience
epochs. The delta which determines what is to be consiered a valid improvement
is determined by plateau_min_d and early_stop_min_d, respectively.

The logs are saved automatically subdirectories named after the timestamp and can be accessed
using TensorBoard. The checkpoints are saved automatically in subdirectories named after
the frontend being adopted and timestamp. The config.json file is copied automatically
in the latter directory for future reference.

IMPORTANT: if trying to profile a batch in TensorBoard, make sure the environment
variable LD_LIBRARY_PATH is specified.
(e.g. 'export LD_LIBRARY_PATH="/usr/local/nvidia/lib:/usr/local/nvidia/lib64:
                               /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64"')


Functions
---------
- train
    Creates a compiled instance of the training model and trains it. 
'''

import datetime
import os
import shutil

import tensorflow as tf

import projectname

def get_compiled_model(frontend, strategy, config, timestamp_to_resume=None):
    ''' Creates a compiled instance of the training model. 
    
    Parameters
    ----------
    frontend: {'waveform', 'log-mel-spectrogram'}
        The frontend to adopt.
        
    strategy: tf.distribute.Strategy
        Strategy for multi-GPU distribution.

    config: argparse.Namespace
        Instance of the config namespace. It is generated when parsing the config.json file.
    
    timestamp_to_resume: str
        Specifies the timestamp of the checkpoint to restore. Should be a timestamp in the 'YYMMDD-hhmm' format.
    '''

    with strategy.scope():
        # read optimizer specs from config namespace to ensure max flexibility
        optimizer = tf.keras.optimizers.get({"class_name": config.optimizer_name, "config": config.optimizer})

        # compile model
        model = projectname.build_model(frontend, num_output_neurons=config.n_output_neurons, num_units=config.n_dense_units, num_filts=config.n_filters, y_input=config.n_mels)
        model.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.SUM), metrics=[[tf.keras.metrics.AUC(curve='ROC', name='AUC-ROC'), tf.keras.metrics.AUC(curve='PR', name='AUC-PR')]])
        
        # restore timestamp_to_resume (if provided)
        if timestamp_to_resume:
            model.load_weights(os.path.join(os.path.join(os.path.expanduser(config.checkpoint_dir), frontend + '_' + timestamp_to_resume)))

    return model

def train(train_dataset, valid_dataset, frontend, strategy, config, epochs, steps_per_epoch=None, timestamp_to_resume=None, update_freq=1, profile_batch=0):
    ''' Creates a compiled instance of the training model and trains it for 'epochs' epochs.

    Parameters
    ----------
    train_dataset: tf.data.Dataset
        The training dataset.
        
    valid_dataset: tf.data.Dataset
        The validation dataset. If None, validation will be disabled. Tfe callbacks might not work properly.

    frontend: {'waveform', 'log-mel-spectrogram'}
        The frontend to adopt.
        
    strategy: tf.distribute.Strategy
        Strategy for multi-GPU distribution.

    config: argparse.Namespace
        Instance of the config namespace. It is generated when parsing the config.json file.
        
    epochs: int
        Specifies the number of epochs to train for.

    steps_per_epoch: int
        Specifies the number of steps to perform for each epoch. If None, the whole dataset will be used.
    
    timestamp_to_resume: str
        Specifies the timestamp of the checkpoint to restore. Should be a timestamp in the 'YYMMDD-hhmm' format.

    update_freq: int
        Specifies the number of batches to wait before writing to logs. Note that writing too frequently can slow down training.

    profile_batch: int
        Specifies the batch to profile. Set to 0 to disable, or if errors occur.
    '''

    timestamp = datetime.datetime.now().strftime("%d%m%y-%H%M")

    # load model
    model = get_compiled_model(frontend, strategy, config, timestamp_to_resume)

    # set up logs and checkpoints directories
    if timestamp_to_resume is None:
        log_dir = os.path.join(os.path.expanduser(config.log_dir), frontend[:13] + '_' + timestamp)
        if not os.path.isdir(log_dir):
            os.makedirs(log_dir)
        shutil.copy(config.path, log_dir) # copy config file in the same folder where the models will be saved
    else:
        log_dir = os.path.join(os.path.expanduser(config.log_dir), frontend[:13] + '_' + timestamp_to_resume) # keep saving logs and checkpoints in the 'old' folder

    # set up callbacks
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath = os.path.join(log_dir, 'mymodel.h5'),
            monitor = 'val_AUC-PR',
            mode = 'max',
            save_best_only = True,
            save_freq = 'epoch',
            verbose = 1,
        ),

        tf.keras.callbacks.TensorBoard(
            log_dir = log_dir,
            histogram_freq = 1,
            write_graph = False,
            update_freq = update_freq,
            profile_batch = profile_batch, # make sure the variable LD_LIBRARY_PATH is properly set up
        ),

        tf.keras.callbacks.TerminateOnNaN(),
    ]

    if config.early_stop_patience is not None:
        # if some parameters have not been provided, use default
        config.early_stop_min_delta = config.early_stop_min_delta or 0
        
        # append callback
        callbacks.append(
            tf.keras.callbacks.EarlyStopping(
                monitor = 'val_AUC-PR',
                mode = 'max',
                min_delta = config.early_stop_min_delta,
                restore_best_weights = True,
                patience = config.early_stop_patience,
                verbose = 1,
            ),
        )
    
    if config.reduceLRoP_patience is not None:
        # if some parameters have not been provided, use default
        config.reduceLRoP_factor = config.reduceLRoP_factor or 0.5
        config.reduceLRoP_min_delta = config.reduceLRoP_min_delta or 0
        config.reduceLRoP_min_lr = config.reduceLRoP_min_lr or 0

        # append callback
        callbacks.append(
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor = 'val_AUC-PR',
                mode = 'max',
                factor = config.reduceLRoP_factor,
                min_delta = config.reduceLRoP_min_delta,
                min_lr = config.reduceLRoP_min_lr,
                patience = config.reduceLRoP_patience,
                verbose = 1,
            ),
        )

    history = model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=callbacks, validation_data=valid_dataset)

    return history.history
