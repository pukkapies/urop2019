''' Contains tools to train our model using the mirrored distribution strategy and a custom training loop.


Notes
-----
This module contains the actual training function used to train a model
using the custom GradientTape API. It combines the data input pipeline 
defined in projectame_input.py (which uses the .tfrecord files previously
generated by audio_processing.py), the CNN architecture proposed 
by Pon et. al (2018) defined in projectname.py, and a custom training loop 
with mirrored strategy integrated for multiple-GPU training.

The training function tries to optimise BinaryCrossentropy for each tag
prediction (batch loss is summed over the batch size, instead of being averaged), and 
displays the area under the ROC and PR curves as metrics. The optimizer can
be fully specified in the config.json file.

The logs are saved automatically subdirectories named after the timestamp and can be accessed
using TensorBoard. The checkpoints are saved automatically in subdirectories named after
the frontend being adopted and timestamp. The config.json file is copied automatically
in the latter directory for future reference. By recovering a checkpoint using resume_time, 
training will resume from the latest completed epoch. 

If early stopping is enabled, a .npy file will be generated to
store the early stopping progress in case the script is stopped and resumed
later.

IMPORTANT: if trying to profile a batch in TensorBoard, make sure the environment
variable LD_LIBRARY_PATH is specified.
(e.g. 'export LD_LIBRARY_PATH="/usr/local/nvidia/lib:/usr/local/nvidia/lib64:
                               /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64"')


Functions
---------
- train
    Creates a compiled instance of the training model and trains it. 
'''

import datetime
import gc
import os
import time
import shutil

import numpy as np
import tensorflow as tf

import projectname
            
def train(train_dataset, valid_dataset, frontend, strategy, config, epochs, steps_per_epoch=None, resume_time=None, update_freq=1, analyse_trace=False):
    ''' Creates a compiled instance of the training model and trains it for 'epochs' epochs.

    Parameters
    ----------
    train_dataset: tf.data.Dataset
        The training dataset.
        
    valid_dataset: tf.data.Dataset
        The validation dataset. If None, validation will be disabled. Tfe callbacks might not work properly.

    frontend: {'waveform', 'log-mel-spectrogram'}
        The frontend to adopt.
        
    strategy: tf.distribute.Strategy
        Strategy for multi-GPU distribution.

    config: argparse.Namespace
        Instance of the config namespace. It is generated when parsing the config.json file.
        
    epochs: int
        Specifies the number of epochs to train for.
    
    resume_time: str
        Specifies the timestamp of the checkpoint to restore. Should be a timestamp in the 'YYMMDD-hhmm' format.

    update_freq: int
        Specifies the number of batches to wait before writing to logs. Note that writing too frequently can slow down training.
    
    analyse_trace: bool
        Specifies whether to enable profiling.
    '''

    log_dir = os.path.join(os.path.expanduser(config.log_dir), 'custom_' + frontend[:13] + '_' + datetime.datetime.now().strftime("%y%m%d-%H%M"))
    
    with strategy.scope():
        
        num_replica = strategy.num_replicas_in_sync
        tf.print('num_replica:', num_replica)
        
        # build model
        model = projectname.build_model(frontend, num_output_neurons=config.n_output_neurons, num_units=config.n_dense_units, num_filts=config.n_filters, y_input=config.n_mels)
        
        # initialise loss, optimizer and metrics
        optimizer = tf.keras.optimizers.get({"class_name": config.optimizer_name, "config": config.optimizer})
        train_loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM)
        train_mean_loss = tf.keras.metrics.Mean(name='train_mean_loss', dtype=tf.float32)
        train_metrics_1 = tf.keras.metrics.AUC(curve='ROC', name='train_AUC-ROC', dtype=tf.float32)
        train_metrics_2 = tf.keras.metrics.AUC(curve='PR', name='train_AUC-PR', dtype=tf.float32)

        # setting up checkpoint
        checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
        prev_epoch = -1
        
        # resume
        if resume_time is None:
            if not os.path.isdir(log_dir):
                os.makedirs(log_dir)
            shutil.copy(config.path, log_dir) # copy config file in the same folder where the models will be saved
        else:
            log_dir = os.path.join(os.path.expanduser(config.log_dir), 'custom_' + frontend[:13] + '_' + resume_time) # keep saving logs and checkpoints in the 'old' folder
            
            # try to load checkpoint
            chkp = tf.train.latest_checkpoint(log_dir)
            if chkp:
                tf.print("Checkpoint file {} found. Restoring...".format(chkp))
                checkpoint.restore(chkp)
                tf.print("Checkpoint restored.")
                prev_epoch = int(chkp.split('-')[-1])-1  #last completed epoch number (from 0)
            else:
                tf.print("Checkpoint file not found!")
                return
        
        tf.summary.trace_off() # in case of previous keyboard interrupt
        
        # setting up summary writers
        train_log_dir = os.path.join(log_dir, 'train/')
        train_summary_writer = tf.summary.create_file_writer(train_log_dir)
        
        if valid_dataset:
            val_log_dir = os.path.join(log_dir, 'validation/')
            val_summary_writer = tf.summary.create_file_writer(val_log_dir)
            val_metrics_1 = tf.keras.metrics.AUC(curve = 'ROC', name='val_AUC-ROC', dtype=tf.float32)
            val_metrics_2 = tf.keras.metrics.AUC(curve = 'PR', name='val_AUC-PR', dtype=tf.float32)
            val_loss = tf.keras.metrics.Mean(name='val_loss', dtype=tf.float32)
        
        if analyse_trace: # make sure the variable LD_LIBRARY_PATH is properly set up
            print('TIPS: To ensure the profiler works correctly, make sure the LD_LIBRARY_PATH is set correctly. \
                  For Boden, set--- export LD_LIBRARY_PATH="/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64" before Python is initialised.')
            prof_log_dir = os.path.join(log_dir, 'profile/')
            prof_summary_writer = tf.summary.create_file_writer(prof_log_dir)
        
        # rescale loss
        def compute_loss(labels, predictions):
            per_example_loss = train_loss(labels, predictions)
            return per_example_loss/config.batch_size
        
        def train_step(batch):
            audio_batch, label_batch = batch
            with tf.GradientTape() as tape:
                logits = model(audio_batch)
                loss = compute_loss(label_batch, logits)
            variables = model.trainable_variables
            grads = tape.gradient(loss, variables)
            optimizer.apply_gradients(zip(grads, variables))

            train_metrics_1.update_state(label_batch, logits)
            train_metrics_2.update_state(label_batch, logits)
            train_mean_loss.update_state(loss)
            return loss

        def valid_step(batch):
            audio_batch, label_batch = batch
            logits = model(audio_batch, training=False)
            loss = compute_loss(label_batch, logits)

            val_metrics_1.update_state(label_batch, logits)
            val_metrics_2.update_state(label_batch, logits)
            val_loss.update_state(loss)
            return loss
            
        @tf.function 
        def distributed_train_body(batch, epoch, num_replica):
            num_batches = 0 
            for batch in train_dataset:
                strategy.experimental_run_v2(train_step, args=(batch, ))
                num_batches += 1
                # print metrics after each iteration
                if tf.equal(num_batches % update_freq, 0):
                    tf.print('Epoch',  epoch,'; Step', num_batches, '; loss', tf.multiply(train_mean_loss.result(), num_replica), '; ROC_AUC', train_metrics_1.result(), ';PR_AUC', train_metrics_2.result())

                    with train_summary_writer.as_default():
                        tf.summary.scalar('ROC_AUC_itr', train_metrics_1.result(), step=optimizer.iterations)
                        tf.summary.scalar('PR_AUC_itr', train_metrics_2.result(), step=optimizer.iterations)
                        tf.summary.scalar('Loss_itr', tf.multiply(train_mean_loss.result(), num_replica), step=optimizer.iterations)
                        train_summary_writer.flush()
                gc.collect()

        @tf.function
        def distributed_val_body(batch):
            for batch in valid_dataset:
                strategy.experimental_run_v2(valid_step, args=(batch, ))
                gc.collect()

        max_metric = -200 # for early stopping

        
        # loop
        for epoch in tf.range(prev_epoch+1, epochs, dtype=tf.int64):
            start_time = time.time()
            tf.print()
            tf.print()
            tf.print('Epoch {}/{}'.format(epoch, epochs-1))
            
            if analyse_trace and tf.equal(epoch, 1):
                tf.summary.trace_off()
                tf.summary.trace_on(graph=False, profiler=True)
            
            distributed_train_body(train_dataset, epoch, num_replica)
            gc.collect()
            
            # write metrics on tensorboard after each epoch
            with train_summary_writer.as_default():
                tf.summary.scalar('ROC_AUC_epoch', train_metrics_1.result(), step=epoch)
                tf.summary.scalar('PR_AUC_epoch', train_metrics_2.result(), step=epoch)
                tf.summary.scalar('mean_loss_epoch', tf.multiply(train_mean_loss.result(), num_replica), step=epoch)
                train_summary_writer.flush()
                
            # print progress
            tf.print('Epoch', epoch,  ': loss', tf.multiply(train_mean_loss.result(), num_replica), '; ROC_AUC', train_metrics_1.result(), '; PR_AUC', train_metrics_2.result())
            
            train_metrics_1.reset_states()
            train_metrics_2.reset_states()
            train_mean_loss.reset_states()

            # export profiling and write validation metrics on tensorboard
            if analyse_trace:
                with prof_summary_writer.as_default():   
                    tf.summary.trace_export(name="trace", 
                                            step=epoch, 
                                            profiler_outdir=os.path.normpath(prof_log_dir)) 

            if valid_dataset:
                distributed_val_body(valid_dataset)
                gc.collect()
                with val_summary_writer.as_default():
                    tf.summary.scalar('ROC_AUC_epoch', val_metrics_1.result(), step=epoch)
                    tf.summary.scalar('PR_AUC_epoch', val_metrics_2.result(), step=epoch)
                    tf.summary.scalar('mean_loss_epoch', tf.multiply(val_loss.result(), num_replica), step=epoch)
                    val_summary_writer.flush()

                tf.print('Val- Epoch', epoch, ': loss', tf.multiply(val_loss.result(), num_replica), ';ROC_AUC', val_metrics_1.result(), '; PR_AUC', val_metrics_2.result())
                
                # early stopping
                if (config.early_stop_min_delta) or (config.early_stop_patience):
                    
                    if not config.early_stop_min_delta:
                        config.early_stop_min_d = 0.
                   
                    if not config.early_stop_patience:
                        config.early_stop_patience = 1
                    
                    if os.path.isfile(os.path.join(log_dir, 'early_stopping.npy')):
                        cumerror = int(np.load(os.path.join(log_dir, 'early_stopping.npy')))
                    
                    if val_metrics_2.result() > (max_metric + config.early_stop_min_delta):
                        max_metric = val_metrics_2.result()
                        cumerror = 0
                        np.save(os.path.join(log_dir, 'early_stopping.npy'), cumerror)
                    else:
                        cumerror += 1
                        tf.print('Epoch {}/{}: no significant improvements ({}/{})'.format(epoch, epochs-1, cumerror, config.early_stop_patience))
                        np.save(os.path.join(log_dir, 'early_stopping.npy'), cumerror)
                        if cumerror == config.early_stop_patience:
                            tf.print('Epoch {}: stopping')
                            break
                
                # reset validation metrics after each epoch
                val_metrics_1.reset_states()
                val_metrics_2.reset_states()
                val_loss.reset_states()
                    
            elif (config.early_stop_min_delta) or (config.early_stop_patience):
                raise RuntimeError('EarlyStopping requires a validation dataset')

            checkpoint_path = os.path.join(log_dir, 'epoch'+str(epoch.numpy()))
            saved_path = checkpoint.save(checkpoint_path)
            tf.print('Saving model as TF checkpoint: {}'.format(saved_path))

            # report time
            time_taken = time.time()-start_time
            tf.print('Epoch {}: {} s'.format(epoch, time_taken))
            
            tf.keras.backend.clear_session()
            gc.collect()
