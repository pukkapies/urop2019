''' Contains tools to train our model using the mirrored distribution strategy.


Notes
-----
This module can be run as a script. To do so, just type 'python train.py' in the terminal. The help 
page should contain all the options you might possibly need.

This module contains the actual training function used to train a model
using the built-in Keras model.fit API, or alternatively a custom training loop. 
It combines the data input pipeline defined in data_input.py (which makes use of 
the .tfrecord files previously generated by preprocessing.py), 
the CNN architecture proposed by Pon et. al (2018) defined in orpheus_model.py, 
and a standard training loop with mirrored strategy 
integrated for multiple-GPU training.

The training function tries to optimise BinaryCrossentropy for each tag
prediction (batch loss is summed over the batch size, instead of being averaged), and 
displays the area under the ROC and PR curves as metrics. The optimizer can
be fully specified in the config.json file.

The learning rate can be halved whenever the validation AUC PR does not improve
for more than plateau_patience epochs. Moreover, training can be automatically
interrupted if the validation AUC PR does not improve for more than early_stop_patience
epochs. In these two cases, the delta which determines what is to be consiered a valid improvement
is determined by plateau_min_d and early_stop_min_d respectively.

Logs and checkpoints are automatically saved in subdirectories named after the 
frontend adopted and a timestamp. 
Logs can be accessed and viewed using TensorBoard. 

The config.json file is automatically copied in the directory for future reference.

IMPORTANT: if trying to profile a batch in TensorBoard, make sure the environment
variable LD_LIBRARY_PATH is specified.
(e.g. 'export LD_LIBRARY_PATH="/usr/local/nvidia/lib:/usr/local/nvidia/lib64:
                               /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64"')
'''

import argparse
import datetime
import gc
import os
import shutil
import time

import numpy as np
import tensorflow as tf

from data_input import generate_datasets_from_dir
from orpheus_model import build_model
from orpheus_model import parse_config_json

def train_with_fit(train_dataset, valid_dataset, frontend, strategy, config, epochs, steps_per_epoch=None, timestamp_to_resume=None, update_freq=1, profile_batch=0):
    ''' Creates a compiled instance of the training model and trains it for 'epochs' epochs.

    Parameters
    ----------
    train_dataset: tf.data.Dataset
        The training dataset.
        
    valid_dataset: tf.data.Dataset
        The validation dataset. If None, validation will be disabled. Tfe callbacks might not work properly.

    frontend: {'waveform', 'log-mel-spectrogram'}
        The frontend to adopt.
        
    strategy: tf.distribute.Strategy
        Strategy for multi-GPU distribution.

    config: argparse.Namespace
        Instance of the config namespace. It is generated when parsing the config.json file.
        
    epochs: int
        Specifies the number of epochs to train for.

    steps_per_epoch: int
        Specifies the number of steps to perform for each epoch. If None, the whole dataset will be used.
    
    timestamp_to_resume: str
        Specifies the timestamp of the checkpoint to restore. Should be a timestamp in the 'YYMMDD-hhmm' format.

    update_freq: int
        Specifies the number of batches to wait before writing to logs. Note that writing too frequently can slow down training.

    profile_batch: int
        Specifies which batch to profile. Set to 0 to disable.
    '''

    timestamp = datetime.datetime.now().strftime("%d%m%y-%H%M")

    with strategy.scope():
        optimizer = tf.keras.optimizers.get({"class_name": config.optimizer_name, "config": config.optimizer})

        # build and compile model
        model = build_model(frontend, num_output_neurons=config.n_output_neurons, num_units=config.n_dense_units, num_filts=config.n_filters, y_input=config.n_mels)
        model.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.SUM), metrics=[[tf.keras.metrics.AUC(curve='ROC', name='AUC-ROC'), tf.keras.metrics.AUC(curve='PR', name='AUC-PR')]])
        
        # restore timestamp_to_resume (if provided)
        if timestamp_to_resume:
            model.load_weights(os.path.join(os.path.join(os.path.expanduser(config.checkpoint_dir), frontend + '_' + timestamp_to_resume)))

    # set up logs and checkpoints directories
    if timestamp_to_resume is None:
        log_dir = os.path.join(os.path.expanduser(config.log_dir), frontend[:13] + '_' + timestamp)
        if not os.path.isdir(log_dir):
            os.makedirs(log_dir)
        shutil.copy(config.path, log_dir) # copy config file in the same folder where the models will be saved
    else:
        log_dir = os.path.join(os.path.expanduser(config.log_dir), frontend[:13] + '_' + timestamp_to_resume) # keep saving logs and checkpoints in the 'old' folder

    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath = os.path.join(log_dir, 'mymodel.h5'),
            monitor = 'val_AUC-PR',
            mode = 'max',
            save_best_only = True,
            save_freq = 'epoch',
            verbose = 1,
        ),

        tf.keras.callbacks.TensorBoard(
            log_dir = log_dir,
            histogram_freq = 1,
            write_graph = False,
            update_freq = update_freq,
            profile_batch = profile_batch, # make sure the variable LD_LIBRARY_PATH is properly set up
        ),

        tf.keras.callbacks.TerminateOnNaN(),
    ]

    if config.early_stop_patience is not None:

        # if some parameters have not been provided, use default
        config.early_stop_min_delta = config.early_stop_min_delta or 0

        callbacks.append(
            tf.keras.callbacks.EarlyStopping(
                monitor = 'val_AUC-PR',
                mode = 'max',
                min_delta = config.early_stop_min_delta,
                restore_best_weights = True,
                patience = config.early_stop_patience,
                verbose = 1,
            ),
        )
    
    if config.reduceLRoP_patience is not None:

        # if some parameters have not been provided, use default
        config.reduceLRoP_factor = config.reduceLRoP_factor or 0.5
        config.reduceLRoP_min_delta = config.reduceLRoP_min_delta or 0
        config.reduceLRoP_min_lr = config.reduceLRoP_min_lr or 0

        callbacks.append(
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor = 'val_AUC-PR',
                mode = 'max',
                factor = config.reduceLRoP_factor,
                min_delta = config.reduceLRoP_min_delta,
                min_lr = config.reduceLRoP_min_lr,
                patience = config.reduceLRoP_patience,
                verbose = 1,
            ),
        )

    model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=callbacks, validation_data=valid_dataset)

    return

def train(train_dataset, valid_dataset, frontend, strategy, config, epochs, steps_per_epoch=None, timestamp_to_resume=None, update_freq=1, analyse_trace=False):
    ''' Creates a compiled instance of the training model and trains it for 'epochs' epochs using a custom training loop.

    Parameters
    ----------
    train_dataset: tf.data.Dataset
        The training dataset.
        
    valid_dataset: tf.data.Dataset
        The validation dataset. If None, validation will be disabled. Tfe callbacks might not work properly.

    frontend: {'waveform', 'log-mel-spectrogram'}
        The frontend to adopt.
        
    strategy: tf.distribute.Strategy
        Strategy for multi-GPU distribution.

    config: argparse.Namespace
        Instance of the config namespace. It is generated when parsing the config.json file.
        
    epochs: int
        Specifies the number of epochs to train for.
    
    timestamp_to_resume: str
        Specifies the timestamp of the checkpoint to restore. Should be a timestamp in the 'YYMMDD-hhmm' format.

    update_freq: int
        Specifies the number of batches to wait before writing to logs. Note that writing too frequently can slow down training.
    
    analyse_trace: bool
        Specifies whether to enable profiling.
    '''

    timestamp = datetime.datetime.now().strftime("%d%m%y-%H%M")
    
    with strategy.scope():
        
        num_replica = strategy.num_replicas_in_sync
        tf.print('num_replica:', num_replica)
        
        # build model
        model = build_model(frontend, num_output_neurons=config.n_output_neurons, num_units=config.n_dense_units, num_filts=config.n_filters, y_input=config.n_mels)
        
        # initialise loss, optimizer and metrics
        optimizer = tf.keras.optimizers.get({"class_name": config.optimizer_name, "config": config.optimizer})
        train_loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM)
        train_mean_loss = tf.keras.metrics.Mean(name='train_mean_loss', dtype=tf.float32)
        train_metrics_1 = tf.keras.metrics.AUC(curve='ROC', name='train_AUC-ROC', dtype=tf.float32)
        train_metrics_2 = tf.keras.metrics.AUC(curve='PR', name='train_AUC-PR', dtype=tf.float32)

        # set up checkpoint
        checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
        prev_epoch = -1
        
       # set up logs and checkpoints directories
        if timestamp_to_resume is None:
            log_dir = os.path.join(os.path.expanduser(config.log_dir), 'custom_' + frontend[:13] + '_' + timestamp)
            if not os.path.isdir(log_dir):
                os.makedirs(log_dir)
            shutil.copy(config.path, log_dir) # copy config file in the same folder where the models will be saved
        else:
            log_dir = os.path.join(os.path.expanduser(config.log_dir), 'custom_' + frontend[:13] + '_' + timestamp_to_resume) # keep saving logs and checkpoints in the 'old' folder
            
            # try to load checkpoint
            chkp = tf.train.latest_checkpoint(log_dir)
            if chkp:
                tf.print("Checkpoint file {} found. Restoring...".format(chkp))
                checkpoint.restore(chkp)
                tf.print("Checkpoint restored.")
                prev_epoch = int(chkp.split('-')[-1])-1  #last completed epoch number (from 0)
            else:
                tf.print("Checkpoint file not found!")
                return
        
        tf.summary.trace_off() # in case of previous keyboard interrupt
        
        # setting up summary writers
        train_log_dir = os.path.join(log_dir, 'train/')
        train_summary_writer = tf.summary.create_file_writer(train_log_dir)
        
        if valid_dataset:
            val_log_dir = os.path.join(log_dir, 'validation/')
            val_summary_writer = tf.summary.create_file_writer(val_log_dir)
            val_metrics_1 = tf.keras.metrics.AUC(curve = 'ROC', name='val_AUC-ROC', dtype=tf.float32)
            val_metrics_2 = tf.keras.metrics.AUC(curve = 'PR', name='val_AUC-PR', dtype=tf.float32)
            val_loss = tf.keras.metrics.Mean(name='val_loss', dtype=tf.float32)
        
        if analyse_trace: # make sure the variable LD_LIBRARY_PATH is properly set up
            print('TIPS: To ensure the profiler works correctly, make sure the LD_LIBRARY_PATH is set correctly. \
                  For Boden, set--- export LD_LIBRARY_PATH="/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64" before Python is initialised.')
            prof_log_dir = os.path.join(log_dir, 'profile/')
            prof_summary_writer = tf.summary.create_file_writer(prof_log_dir)
        
        # rescale loss
        def compute_loss(labels, predictions):
            per_example_loss = train_loss(labels, predictions)
            return per_example_loss/config.batch_size
        
        def train_step(batch):
            audio_batch, label_batch = batch
            with tf.GradientTape() as tape:
                logits = model(audio_batch)
                loss = compute_loss(label_batch, logits)
            variables = model.trainable_variables
            grads = tape.gradient(loss, variables)
            optimizer.apply_gradients(zip(grads, variables))

            train_metrics_1.update_state(label_batch, logits)
            train_metrics_2.update_state(label_batch, logits)
            train_mean_loss.update_state(loss)
            return loss

        def valid_step(batch):
            audio_batch, label_batch = batch
            logits = model(audio_batch, training=False)
            loss = compute_loss(label_batch, logits)

            val_metrics_1.update_state(label_batch, logits)
            val_metrics_2.update_state(label_batch, logits)
            val_loss.update_state(loss)
            return loss
            
        @tf.function 
        def distributed_train_body(batch, epoch, num_replica):
            num_batches = 0 
            for batch in train_dataset:
                strategy.experimental_run_v2(train_step, args=(batch, ))
                num_batches += 1
                # print metrics after each iteration
                if tf.equal(num_batches % update_freq, 0):
                    tf.print('Epoch',  epoch,'; Step', num_batches, '; loss', tf.multiply(train_mean_loss.result(), num_replica), '; ROC_AUC', train_metrics_1.result(), ';PR_AUC', train_metrics_2.result())

                    with train_summary_writer.as_default():
                        tf.summary.scalar('ROC_AUC_itr', train_metrics_1.result(), step=optimizer.iterations)
                        tf.summary.scalar('PR_AUC_itr', train_metrics_2.result(), step=optimizer.iterations)
                        tf.summary.scalar('Loss_itr', tf.multiply(train_mean_loss.result(), num_replica), step=optimizer.iterations)
                        train_summary_writer.flush()
                gc.collect()

        @tf.function
        def distributed_val_body(batch):
            for batch in valid_dataset:
                strategy.experimental_run_v2(valid_step, args=(batch, ))
                gc.collect()

        max_metric = -200 # for early stopping

        # loop
        for epoch in tf.range(prev_epoch+1, epochs, dtype=tf.int64):
            start_time = time.time()
            tf.print()
            tf.print()
            tf.print('Epoch {}/{}'.format(epoch, epochs-1))
            
            if analyse_trace and tf.equal(epoch, 1):
                tf.summary.trace_off()
                tf.summary.trace_on(graph=False, profiler=True)
            
            distributed_train_body(train_dataset, epoch, num_replica)
            gc.collect()
            
            # write metrics on tensorboard after each epoch
            with train_summary_writer.as_default():
                tf.summary.scalar('ROC_AUC_epoch', train_metrics_1.result(), step=epoch)
                tf.summary.scalar('PR_AUC_epoch', train_metrics_2.result(), step=epoch)
                tf.summary.scalar('mean_loss_epoch', tf.multiply(train_mean_loss.result(), num_replica), step=epoch)
                train_summary_writer.flush()
                
            # print progress
            tf.print('Epoch', epoch,  ': loss', tf.multiply(train_mean_loss.result(), num_replica), '; ROC_AUC', train_metrics_1.result(), '; PR_AUC', train_metrics_2.result())
            
            train_metrics_1.reset_states()
            train_metrics_2.reset_states()
            train_mean_loss.reset_states()

            # write training profile
            if analyse_trace:
                with prof_summary_writer.as_default():   
                    tf.summary.trace_export(name="trace", 
                                            step=epoch, 
                                            profiler_outdir=os.path.normpath(prof_log_dir)) 

            if valid_dataset:
                distributed_val_body(valid_dataset)
                gc.collect()

                # write metris on tensorboard after each epoch
                with val_summary_writer.as_default():
                    tf.summary.scalar('ROC_AUC_epoch', val_metrics_1.result(), step=epoch)
                    tf.summary.scalar('PR_AUC_epoch', val_metrics_2.result(), step=epoch)
                    tf.summary.scalar('mean_loss_epoch', tf.multiply(val_loss.result(), num_replica), step=epoch)
                    val_summary_writer.flush()

                tf.print('Val- Epoch', epoch, ': loss', tf.multiply(val_loss.result(), num_replica), ';ROC_AUC', val_metrics_1.result(), '; PR_AUC', val_metrics_2.result())
                
                # early stopping callback
                if config.early_stop_patience is not None:
                    
                    # if some parameters have not been provided, use default
                    config.early_stop_min_delta = config.early_stop_min_delta or 0.
                    
                    if os.path.isfile(os.path.join(log_dir, 'early_stopping.npy')):
                        cumerror = int(np.load(os.path.join(log_dir, 'early_stopping.npy')))

                    if val_metrics_2.result() > (max_metric + config.early_stop_min_delta):
                        max_metric = val_metrics_2.result()
                        cumerror = 0
                        np.save(os.path.join(log_dir, 'early_stopping.npy'), cumerror)
                    else:
                        cumerror += 1
                        tf.print('Epoch {}/{}: no significant improvements ({}/{})'.format(epoch, epochs-1, cumerror, config.early_stop_patience))
                        np.save(os.path.join(log_dir, 'early_stopping.npy'), cumerror)
                        if cumerror == config.early_stop_patience:
                            tf.print('Epoch {}: stopping')
                            break
                
                # reset validation metrics after each epoch
                val_metrics_1.reset_states()
                val_metrics_2.reset_states()
                val_loss.reset_states()
                    
            elif config.early_stop_patience is not None:
                raise RuntimeError('EarlyStopping requires a validation dataset')

            checkpoint_path = os.path.join(log_dir, 'epoch'+str(epoch.numpy()))
            saved_path = checkpoint.save(checkpoint_path)
            tf.print('Saving model as TF checkpoint: {}'.format(saved_path))

            # report time
            time_taken = time.time()-start_time
            tf.print('Epoch {}: {} s'.format(epoch, time_taken))

    return

if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    parser.add_argument('frontend', choices=['waveform', 'log-mel-spectrogram'])
    parser.add_argument('--tfrecords-dir', help='directory to read the .tfrecord files from', required=True)
    parser.add_argument('--config', help='path to config.json (default to path on Boden)', default='/srv/data/urop/config.json')
    parser.add_argument('--lastfm', help='path to (clean) lastfm database (default to path on Boden)', default='/srv/data/urop/clean_lastfm.db')
    parser.add_argument('--multi-db', help='specify the number of different tags features in the .tfrecord files', type=int, default=1)
    parser.add_argument('--multi-db-default', help='specify the index of the default tags database, when there are more than one tags features in the .tfrecord files', type=int)
    parser.add_argument('--epochs', help='specify the number of epochs to train for', type=int, default=1)
    parser.add_argument('--steps-per-epoch', help='specify the number of steps to perform at each epoch (if unspecified, go through the whole dataset)', type=int)
    parser.add_argument('--no-shuffle', action='store_true', help='force no shuffle, override config setting')
    parser.add_argument('--resume', help='load a previously saved model with the time in the format ddmmyy-hhmm, e.g. if the folder which the model is saved is custom_log-mel-spect_160919-0539, resume should take the argument 160919-0539')
    parser.add_argument('--update-freq', help='specify the frequency (in steps) to record metrics and losses', type=int, default=10)
    parser.add_argument('--cuda', help='set cuda visible devices', type=int, nargs='+')
    parser.add_argument('--built-in', action='store_true', help='train using the built-in model.fit training loop')
    parser.add_argument('-v', '--verbose', choices=['0', '1', '2', '3'], help='verbose mode', default='0')

    args = parser.parse_args()

    # specify number of visible gpu's
    if args.cuda:
        os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' # see issue #152
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in args.cuda])

    # specify verbose mode
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = args.verbose

    # parse config
    config = parse_config_json(args.config, args.lastfm)

    # override config setting
    if args.no_shuffle:
        config.shuffle = False

    # generate train_dataset and valid_dataset (valid_dataset will be None if config.split is None)
    train_dataset, valid_dataset = generate_datasets_from_dir(args.tfrecords_dir, args.frontend, split = config.split, which_split=(True, True, ) + (False, ) * (len(config.split)-2),
                                                              sample_rate = config.sample_rate, batch_size = config.batch_size, 
                                                              block_length = config.interleave_block_length, cycle_length = config.interleave_cycle_length,
                                                              shuffle = config.shuffle, shuffle_buffer_size = config.shuffle_buffer_size, 
                                                              window_length = config.window_length, window_random = config.window_random, 
                                                              num_mels = config.n_mels, num_tags = config.n_tags, num_tags_db = args.multi_db, default_tags_db = args.multi_db_default, with_tags = config.tags, merge_tags = config.tags_to_merge,
										                      as_tuple = True)

    # set up training strategy
    strategy = tf.distribute.MirroredStrategy()

    if not args.built_in:
        # datasets need to be manually 'distributed'
        train_dataset = strategy.experimental_distribute_dataset(train_dataset)
        if valid_dataset is not None:
            valid_dataset = strategy.experimental_distribute_dataset(valid_dataset)
        
        # train model using custom training loop (default choice)
        train(train_dataset, valid_dataset, frontend=args.frontend,
                strategy=strategy, epochs=args.epochs, steps_per_epoch=args.steps_per_epoch, 
                config=config,
                update_freq=args.update_freq, timestamp_to_resume=args.resume)
    else:
        # train model
        train_with_fit(train_dataset, valid_dataset, frontend=args.frontend,
                strategy=strategy, epochs=args.epochs, steps_per_epoch=args.steps_per_epoch, 
                config=config,
                update_freq=args.update_freq, timestamp_to_resume=args.resume)
