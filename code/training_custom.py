''' Contains tools to train our model using the mirrored distribution strategy and a custom training loop.


Notes
-----
This module contains the actual training function used to train a model
using the custom GradientTape API. It combines the data input pipeline 
defined in projectame_input.py (which uses the .tfrecord files previously
generated by audio_processing.py), the CNN architecture proposed 
by Pon et. al (2018) defined in projectname.py, and a custom training loop 
with mirrored strategy integrated for multiple-GPU training.

The training function tries to optimise BinaryCrossentropy for each tag
prediction (batch loss is summed over the batch size, instead of being averaged), and 
displays the area under the ROC and PR curves as metrics. The optimizer can
be fully specified in the config.json file.

The logs are saved automatically subdirectories named after the timestamp and can be accessed
using TensorBoard. The checkpoints are saved automatically in subdirectories named after
the frontend being adopted and timestamp. The config.json file is copied automatically
in the latter directory for future reference. By recovering a checkpoint using timestamp_to_resume, 
training will resume from the latest completed epoch. 

If early stopping is enabled, a .npy file will be generated to
store the early stopping progress in case the script is stopped and resumed
later.

IMPORTANT: if trying to profile a batch in TensorBoard, make sure the environment
variable LD_LIBRARY_PATH is specified.
(e.g. 'export LD_LIBRARY_PATH="/usr/local/nvidia/lib:/usr/local/nvidia/lib64:
                               /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64"')


Functions
---------
- train
    Creates a compiled instance of the training model and trains it. 
'''

import datetime
import gc
import json
import os
import time
import shutil

import numpy as np
import tensorflow as tf

import projectname
            
def train(train_dataset, valid_dataset, frontend, strategy, config, epochs, steps_per_epoch=None, lr_range=None, timestamp_to_resume=None, update_freq=1, analyse_trace=False):
    ''' Creates a compiled instance of the training model and trains it for 'epochs' epochs.

    Parameters
    ----------
    train_dataset: tf.data.Dataset
        The training dataset.
        
    valid_dataset: tf.data.Dataset
        The validation dataset. If None, validation will be disabled. Tfe callbacks might not work properly.

    frontend: {'waveform', 'log-mel-spectrogram'}
        The frontend to adopt.
        
    strategy: tf.distribute.Strategy
        Strategy for multi-GPU distribution.

    config: argparse.Namespace
        Instance of the config namespace. It is generated when parsing the config.json file.
        
    epochs: int
        Specifies the number of epochs to train for.
    
    timestamp_to_resume: str
        Specifies the timestamp of the checkpoint to restore. Should be a timestamp in the 'YYMMDD-hhmm' format.

    update_freq: int
        Specifies the number of batches to wait before writing to logs. Note that writing too frequently can slow down training.
    
    analyse_trace: bool
        Specifies whether to enable profiling.
    '''

    timestamp = datetime.datetime.now().strftime("%d%m%y-%H%M")
    
    with strategy.scope():
        
        num_replica = strategy.num_replicas_in_sync
        tf.print('num_replica:', num_replica)
        
        # build model
        model = projectname.build_model(frontend, num_output_neurons=config.n_output_neurons, num_units=config.n_dense_units, num_filts=config.n_filters, y_input=config.n_mels)

        def get_cyclic_learning_rate(step, iterations, max_lr):
            # it is recommended that min_lr is 1/3 or 1/4th of the maximum lr. see:  
            min_lr = max_lr/4
            # cycle stepsize twice the iterations in an epoch is recommended
            cycle_stepsize = iterations*2

            current_cycle = tf.floor(step/(2*cycle_stepsize))
            ratio = step/cycle_stepsize-current_cycle*2
            lr = min_lr + (max_lr - min_lr)*tf.cast(tf.abs(tf.abs(ratio-1)-1), dtype=tf.float32)
            return lr

        def get_range_test_learning_rate(step, lr_range, iterations):
            lr = lr_range[0]*tf.cast((lr_range[1]/lr_range[0])**(step/iterations), dtype=tf.float32)
            return lr

        if lr_range:
            config_optim.config['learning_rate'] = tf.Variable(get_range_test_learning_rate(0, lr_range, config.iterations))
        elif config_optim.max_learning_rate:
            config_optim.config['learning_rate'] = tf.Variable(get_cyclic_learning_rate(0, config.iterations, config_optim.max_learning_rate))

        # initialise loss, optimizer and metrics
        optimizer = tf.keras.optimizers.get({"class_name": config.optimizer_name, "config": config.optimizer})

        train_loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM)
        train_mean_loss = tf.keras.metrics.Mean(name='train_mean_loss', dtype=tf.float32)
        train_metrics_1 = tf.keras.metrics.AUC(curve='ROC', name='train_AUC-ROC', dtype=tf.float32)
        train_metrics_2 = tf.keras.metrics.AUC(curve='PR', name='train_AUC-PR', dtype=tf.float32)

        # set up checkpoint
        checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
        prev_epoch = -1
        
       # set up logs and checkpoints directories
        if timestamp_to_resume is None:
            log_dir = os.path.join(os.path.expanduser(config.log_dir), 'custom_' + frontend[:13] + '_' + timestamp)
            if not os.path.isdir(log_dir):
                os.makedirs(log_dir)
            shutil.copy(config.path, log_dir) # copy config file in the same folder where the models will be saved
        else:
            log_dir = os.path.join(os.path.expanduser(config.log_dir), 'custom_' + frontend[:13] + '_' + timestamp_to_resume) # keep saving logs and checkpoints in the 'old' folder
            
            # try to load checkpoint
            chkp = tf.train.latest_checkpoint(log_dir)
            if chkp:
                tf.print("Checkpoint file {} found. Restoring...".format(chkp))
                checkpoint.restore(chkp)
                tf.print("Checkpoint restored.")
                prev_epoch = int(chkp.split('-')[-1])-1  #last completed epoch number (from 0)
            else:
                tf.print("Checkpoint file not found!")
                return
        
        tf.summary.trace_off() # in case of previous keyboard interrupt
        
        # setting up summary writers
        train_log_dir = os.path.join(log_dir, 'train/')
        train_summary_writer = tf.summary.create_file_writer(train_log_dir)
        
        if valid_dataset:
            val_log_dir = os.path.join(log_dir, 'validation/')
            val_summary_writer = tf.summary.create_file_writer(val_log_dir)
            val_metrics_1 = tf.keras.metrics.AUC(curve = 'ROC', name='val_AUC-ROC', dtype=tf.float32)
            val_metrics_2 = tf.keras.metrics.AUC(curve = 'PR', name='val_AUC-PR', dtype=tf.float32)
            val_loss = tf.keras.metrics.Mean(name='val_loss', dtype=tf.float32)
        
        if analyse_trace: # make sure the variable LD_LIBRARY_PATH is properly set up
            print('TIPS: To ensure the profiler works correctly, make sure the LD_LIBRARY_PATH is set correctly. \
                  For Boden, set--- export LD_LIBRARY_PATH="/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64" before Python is initialised.')
            prof_log_dir = os.path.join(log_dir, 'profile/')
            prof_summary_writer = tf.summary.create_file_writer(prof_log_dir)

        
        # rescale loss
        def compute_loss(labels, predictions):
            per_example_loss = train_loss(labels, predictions)
            return per_example_loss/config.batch_size
        
        def train_step(batch):
            audio_batch, label_batch = batch
            with tf.GradientTape() as tape:
                logits = model(audio_batch)
                loss = compute_loss(label_batch, logits)
            variables = model.trainable_variables
            grads = tape.gradient(loss, variables)
            optimizer.apply_gradients(zip(grads, variables))

            train_metrics_1.update_state(label_batch, logits)
            train_metrics_2.update_state(label_batch, logits)
            train_mean_loss.update_state(loss)
            return loss

        def valid_step(batch):
            audio_batch, label_batch = batch
            logits = model(audio_batch, training=False)
            loss = compute_loss(label_batch, logits)

            val_metrics_1.update_state(label_batch, logits)
            val_metrics_2.update_state(label_batch, logits)
            val_loss.update_state(loss)
            return loss
            
        @tf.function 
        def distributed_train_body(entry, epoch, num_replica):
            for entry in train_dataset:
                per_replica_losses = strategy.experimental_run_v2(train_step, args=(entry, ))

                if lr_range:
                    optimizer.learning_rate.assign(get_range_test_learning_rate(optimizer.iterations, lr_range, config.iterations))
                elif config_optim.max_learning_rate:
                    optimizer.learning_rate.assign(get_cyclic_learning_rate(optimizer.iterations, config.iterations, config_optim.max_learning_rate))

                # print metrics after each iteration
                if tf.equal(optimizer.iterations % update_freq, 0):
                    tf.print('Epoch',  epoch,'; Step', optimizer.iterations, '; loss', tf.multiply(train_mean_loss.result(), num_replica), 
                             '; ROC_AUC', train_metrics_1.result(), ';PR_AUC', train_metrics_2.result(), '; learning rate', optimizer.learning_rate)

                    with train_summary_writer.as_default():
                        tf.summary.scalar('ROC_AUC_itr', train_metrics_1.result(), step=optimizer.iterations)
                        tf.summary.scalar('PR_AUC_itr', train_metrics_2.result(), step=optimizer.iterations)
                        if lr_range:
                            tf.summary.scalar('Learning rate', optimizer.learning_rate, step=optimizer.iterations)
                            tf.summary.scalar('Loss_itr', strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None), step=optimizer.iterations)
                        else:
                            tf.summary.scalar('Loss_itr', tf.multiply(train_mean_loss.result(), num_replica), step=optimizer.iterations)

                        train_summary_writer.flush()

        @tf.function
        def distributed_val_body(entry):
            for entry in valid_dataset:
                strategy.experimental_run_v2(valid_step, args=(entry, ))

        max_metric = -200 # for early stopping

        # loop
        for epoch in tf.range(prev_epoch+1, epochs, dtype=tf.int64):
            start_time = time.time()
            tf.print()
            tf.print()
            tf.print('Epoch {}/{}'.format(epoch, epochs-1))
            
            if analyse_trace and tf.equal(epoch, 1):
                tf.summary.trace_off()
                tf.summary.trace_on(graph=False, profiler=True)
            
            distributed_train_body(train_dataset, epoch, num_replica)
            
            # write metrics on tensorboard after each epoch
            with train_summary_writer.as_default():
                tf.summary.scalar('ROC_AUC_epoch', train_metrics_1.result(), step=epoch)
                tf.summary.scalar('PR_AUC_epoch', train_metrics_2.result(), step=epoch)
                tf.summary.scalar('mean_loss_epoch', tf.multiply(train_mean_loss.result(), num_replica), step=epoch)
                train_summary_writer.flush()
                
            # print progress
            tf.print('Epoch', epoch,  ': loss', tf.multiply(train_mean_loss.result(), num_replica), '; ROC_AUC', train_metrics_1.result(), '; PR_AUC', train_metrics_2.result())
            
            train_metrics_1.reset_states()
            train_metrics_2.reset_states()
            train_mean_loss.reset_states()

            # write training profile
            if analyse_trace:
                with prof_summary_writer.as_default():   
                    tf.summary.trace_export(name="trace", 
                                            step=epoch, 
                                            profiler_outdir=os.path.normpath(prof_log_dir)) 

            if valid_dataset:

                distributed_val_body(valid_dataset)
                with val_summary_writer.as_default():
                    tf.summary.scalar('ROC_AUC_epoch', val_metrics_1.result(), step=epoch)
                    tf.summary.scalar('PR_AUC_epoch', val_metrics_2.result(), step=epoch)
                    tf.summary.scalar('mean_loss_epoch', tf.multiply(val_loss.result(), num_replica), step=epoch)
                    val_summary_writer.flush()

                tf.print('Val- Epoch', epoch, ': loss', tf.multiply(val_loss.result(), num_replica), ';ROC_AUC', val_metrics_1.result(), '; PR_AUC', val_metrics_2.result())
                
                # early stopping callback
                if config.early_stop_patience is not None:

                    # if some parameters have not been provided, use default
                    config.early_stop_min_delta = config.early_stop_min_delta or 0.
                    
                    if os.path.isfile(os.path.join(checkpoint_dir, 'early_stopping.npy')):
                        cumerror = int(np.load(os.path.join(checkpoint_dir, 'early_stopping.npy')))

                    if tf.less(config_optim.min_lr_plateau, optimizer.learning_rate):
                        if config_optim.lr_plateau_mult:
                            optimizer.learning_rate.assign(tf.multiply(optimizer.learning_rate, config_optim.lr_plateau_mult))
                        else:
                            optimizer.learning_rate.assign(tf.multiply(optimizer.learning_rate, ))

                    elif val_metrics_2.result() > (max_metric + config.early_stop_min_d):
                        max_metric = val_metrics_2.result()
                        cumerror = 0
                        np.save(os.path.join(log_dir, 'early_stopping.npy'), cumerror)
                    else:
                        cumerror += 1
                        tf.print('Epoch {}/{}: no significant improvements ({}/{})'.format(epoch, epochs-1, cumerror, config.early_stop_patience))
                        np.save(os.path.join(log_dir, 'early_stopping.npy'), cumerror)
                        if cumerror == config.early_stop_patience:
                            tf.print('Epoch {}: stopping')
                            break
                
                # reset validation metrics after each epoch
                val_metrics_1.reset_states()
                val_metrics_2.reset_states()
                val_loss.reset_states()
                    
            elif config.early_stop_patience is not None:
                raise RuntimeError('EarlyStopping requires a validation dataset')

            checkpoint_path = os.path.join(log_dir, 'epoch'+str(epoch.numpy()))
            saved_path = checkpoint.save(checkpoint_path)
            tf.print('Saving model as TF checkpoint: {}'.format(saved_path))

            # report time
            time_taken = time.time()-start_time
            tf.print('Epoch {}: {} s'.format(epoch, time_taken))
            
            tf.keras.backend.clear_session()
            gc.collect()
